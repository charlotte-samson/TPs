{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environnement\n",
    "============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SICK_dev_logistic.txt', 'SICK_test.txt', 'SICK_train_logistic.txt']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.data import Field,ReversibleField,TabularDataset,Iterator, BucketIterator\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "Lecture des données\n",
    "===================\n",
    "Pour de plus amples informations, on recommande la consultation de :\n",
    "https://mlexplained.com/2018/02/08/a-comprehensive-tutorial-to-torchtext/ \n",
    "et de:\n",
    "https://torchtext.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/glove.6B.zip: 862MB [06:28, 2.22MB/s]                          \n",
      "100%|█████████▉| 399747/400000 [00:29<00:00, 13719.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8108 A topless woman is being covered in mud || A topless woman is being smeared with a brown substance and a blurry crowd is in the background 0.76\n",
      "6160 A race car driver is standing up and pointing his hand at the sky || The person in the blue jacket is wearing a colorful helmet 0.36\n",
      "1670 A boat is sailing peacefully over the water || The guitar is being played by a man 0.2\n",
      "415 Two men with bikes are on the side of a snowy road || Two men with cars are on the side of a snowy road 0.7\n",
      "7511 There are no dogs fighting for a frisbee in a lake || Two brown dogs are playing with a frisbee in the water 0.78\n",
      "3777 A woman is dancing and a man is playing the keyboard || The woman is dancing and a man is playing the keyboard 0.9800000000000001\n",
      "3574 Paper is being cut with scissors || The piece of paper is being cut 0.8800000000000001\n",
      "7213 A brown dog is running breathlessly across the yard with a toy in its mouth || A brown dog is running across the yard with a toy in its mouth 0.9\n",
      "4340 A man is picking a can || A woman is slicing a tomato 0.27999999999999997\n",
      "9532 Two race cars are on a road in front of a grassy parking area || Two cars for racing are on a road in front of a grassy parking area 0.9800000000000001\n",
      "9979 A person is wearing a straw hat and smoking a cigarette || A dog is near the red ball in the air\n",
      "9986 A woman is riding a horse || A boy in blue are sliding down a green slide\n",
      "9987 A man is drawing some figures || A man is strolling in the rain\n",
      "9989 The people are walking on the road beside a beautiful waterfall || There is no brown dog and black dog playing in the sand\n",
      "9990 A cheetah is chasing its prey across a field || A white and brown dog is walking through the water with difficulty\n",
      "9991 The young girl is blowing a bubble that is huge || There is no girl in pink twirling a ribbon\n",
      "9992 A dog in a colored coat is running across the yard || The flute is being played by one man\n",
      "9994 A boy is happily playing the piano || A white bird is landing swiftly in the water\n",
      "9995 The girl , who is little , is combing her hair into a pony tail . || Two people wearing helmets are driving over the yellow and white flowers\n",
      "9996 A man is in a parking lot and is playing tennis against a large wall || The snowboarder is leaping fearlessly over white snow\n",
      "4927\n"
     ]
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    return text.split()\n",
    "\n",
    "def proc_float(value):\n",
    "    return float(value)\n",
    "\n",
    "def proc_int(value):\n",
    "    return int(value)\n",
    "\n",
    "TEXT      = Field(sequential=True, tokenize=tokenize) #might alternatively specify cuda data types to get the dataset to live permanently on the GPU\n",
    "FLOAT     = Field(sequential=False, use_vocab=False,dtype=torch.float,preprocessing=proc_float) \n",
    "INTEGER   = Field(sequential=False, use_vocab=False,preprocessing=proc_int)\n",
    "\n",
    "df         = TabularDataset(\"../input/SICK_train_logistic.txt\",\"tsv\",skip_header=True,\\\n",
    "                            fields=[('idx',INTEGER),('sentA',TEXT),('sentB',TEXT),('Relatedness',FLOAT)])\n",
    "df_train,df_dev  = df.split(split_ratio=0.8)\n",
    "#TEXT.build_vocab(df_train)\n",
    "#glove\n",
    "TEXT.build_vocab(df_train,vectors='glove.6B.100d')\n",
    "\n",
    "#Prints out the first few lines of the dataset\n",
    "for elt in df_dev[:10]: #prints out the ten first examples\n",
    "    print(elt.idx,' '.join(elt.sentA),'||',' '.join(elt.sentB),elt.Relatedness)\n",
    "    \n",
    "df_test = TabularDataset(\"../input/SICK_test.txt\",\"tsv\",skip_header=True,fields=[('idx',INTEGER),('sentA',TEXT),('sentB',TEXT)])\n",
    "for elt in df_test[-10:]: #prints out the ten first examples\n",
    "    print(elt.idx,' '.join(elt.sentA),'||',' '.join(elt.sentB))\n",
    "print(len(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification\n",
    "=============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 399747/400000 [00:40<00:00, 13719.74it/s]"
     ]
    }
   ],
   "source": [
    "class ParaphraseClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self,hidden_dim,embedding_dim):\n",
    "\n",
    "        super(ParaphraseClassifier, self).__init__()\n",
    "        \n",
    "        self.hidden_dim    = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding     = nn.Embedding(len(TEXT.vocab), embedding_dim)\n",
    "        self.embedding.weight.data.copy_(TEXT.vocab.vectors)\n",
    "        self.embedding.weight.requires_grad=False\n",
    "        self.lstm          = nn.LSTM(embedding_dim, hidden_dim, num_layers=1,bidirectional=False)\n",
    "        self.Wadd          = nn.Linear(hidden_dim,hidden_dim)   \n",
    "        self.Wtimes        = nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.Wout          = nn.Linear(hidden_dim,1)\n",
    "\n",
    "\n",
    "    def forward(self,xinputA,xinputB):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            xinputA is a sequence of word indexes\n",
    "            xinputB is a sequence of word indexes\n",
    "        The forward method also works for batched input.       \n",
    "        \"\"\"\n",
    "        ##details for dimensionalities\n",
    "        #embeddings\n",
    "        #  input : batch_size x seq_length\n",
    "        #  output: batch-size x seq_length x embedding_dimension\n",
    "        #lstm\n",
    "        #  input : seq_length x batch_size x embedding_size\n",
    "        #  output: seq_length x batch_size x hidden_size  (for the sequence)\n",
    "        #  output: batch_size x hidden_size (for the last hidden/cell state)\n",
    "                \n",
    "        xembeddedA                       = self.embedding(xinputA)                                                #catches embedding vectors\n",
    "        lstm_outA, (hiddenA,cellA)       = self.lstm(xembeddedA.view(len(xinputA), -1, self.embedding_dim), None) #-1 is a wildcard (here we let pytorch guess batch size)\n",
    "\n",
    "        xembeddedB                       = self.embedding(xinputB)                                                #catches embedding vectors\n",
    "        lstm_outB, (hiddenB,cellB)       = self.lstm(xembeddedB.view(len(xinputB), -1, self.embedding_dim), None)\n",
    "\n",
    "        #hiddenA = hiddenA.view(-1,self.hidden_dim * 2)\n",
    "        #hiddenB = hiddenB.view(-1,self.hidden_dim * 2)       \n",
    "        #merge sentence representations\n",
    "        hiddenT = hiddenA * hiddenB\n",
    "        hiddenD = torch.abs(hiddenA - hiddenB)\n",
    "        hidden  = torch.tanh(self.Wtimes(hiddenT) + self.Wadd(hiddenD))\n",
    "        return torch.sigmoid(self.Wout(hidden))\n",
    "\n",
    "    def run_train(self,train_set,dev_set,epochs,learning_rate=0.001):\n",
    "\n",
    "        loss_func  = nn.BCELoss() \n",
    "        optimizer  = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "                \n",
    "        train_iterator,dev_iterator = BucketIterator.splits((train_set,dev_set),batch_sizes=(64,1),device=-1,sort_key=lambda x: len(x.sentA),sort_within_batch=False,repeat=False)\n",
    "       \n",
    "        bestloss=1\n",
    "        for e in range(epochs):\n",
    "            for batch in train_iterator: \n",
    "                xvecA,xvecB,yRelness = batch.sentA,batch.sentB,batch.Relatedness\n",
    "                self.zero_grad()\n",
    "                prob            = self.forward(xvecA,xvecB).squeeze()\n",
    "                loss            = loss_func(prob,yRelness)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            for batchdev in dev_iterator: \n",
    "                xvecA,xvecB,yRelness = batchdev.sentA,batchdev.sentB,batchdev.Relatedness\n",
    "                pred= self.forward(xvecA,xvecB).squeeze().view(1)\n",
    "                lossnow = loss_func(pred,yRelness)\n",
    "                #print(pred.shape,yRelness.shape,\"dev\")\n",
    "                if (lossnow < bestloss) :\n",
    "                    bestloss=lossnow\n",
    "                    torch.save(self.state_dict(),\"params.pth\")\n",
    "        self.load_state_dict(torch.load(\"params.pth\"))\n",
    "        \n",
    "    def run_test(self,test_set) : \n",
    "        self.eval()\n",
    "        self.load_state_dict(torch.load(\"params.pth\"))\n",
    "        test_iterator = Iterator(test_set, batch_size=1, device=-1, sort=False, sort_within_batch=False, repeat=False)\n",
    "        pred=[]\n",
    "        idx=[]\n",
    "        for batch in test_iterator: \n",
    "                xvecA,xvecB = batch.sentA,batch.sentB\n",
    "                pred.append((self.forward(xvecA,xvecB).squeeze().item()/2)*10)\n",
    "                idx.append(batch.idx.item())\n",
    "        submission = pd.DataFrame({\"pairID\" : idx ,\"Relatedness\" : pred})\n",
    "        #enregistre les preds\n",
    "        filename = 'DugrainCharlotte_SICK.csv'\n",
    "        submission.to_csv(filename,index=False)\n",
    "        return pred,submission\n",
    "\n",
    "pc = ParaphraseClassifier(150,100)\n",
    "pc.run_train(df_train,df_dev,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred,submission=pc.run_test(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pairID</th>\n",
       "      <th>Relatedness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3698</td>\n",
       "      <td>4.151008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5731</td>\n",
       "      <td>4.087580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>6055</td>\n",
       "      <td>4.428403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2880</td>\n",
       "      <td>4.686611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7536</td>\n",
       "      <td>4.726976</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pairID  Relatedness\n",
       "0    3698     4.151008\n",
       "1    5731     4.087580\n",
       "2    6055     4.428403\n",
       "3    2880     4.686611\n",
       "4    7536     4.726976"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
