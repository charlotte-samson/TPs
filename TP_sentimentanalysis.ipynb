{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environnement\n",
    "============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sentimentIMDB_train.csv', 'sentimentIMDB_test.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "print(os.listdir(\"../input\"))\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chargement des données\n",
    "======================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 25000 examples as train set. \n",
      "Train data Sample (1st batch only)\n",
      "   This movie is very cool. If you're a fan of Tsui H ... 1\n",
      "   Simply put, this is the worst movie since \"Police  ... 0\n",
      "   I cannot say enough bad things about this train wr ... 0\n",
      "   I've just had the evidence that confirmed my suspi ... 0\n",
      "Loaded 25000 examples as test set. \n"
     ]
    }
   ],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This is a subclass of torch.utils.data Dataset and it implements \n",
    "    methods that make the dataset compatible with pytorch data utilities, notably the DataLoader\n",
    "    \"\"\"\n",
    "    def __init__(self,datalines):\n",
    "        self.xydata = datalines\n",
    "    \n",
    "    def __len__(self):              #API requirement\n",
    "        return len(self.xydata)\n",
    "    \n",
    "    def __getitem__(self,idx):      #API requirement\n",
    "        return self.xydata[idx]\n",
    "\n",
    "def load_data_set(filename):\n",
    "    \"\"\"\n",
    "    Loads a dataset as a list of tuples: (text,label)\n",
    "    Args:\n",
    "       filename (str): the dataset filename \n",
    "    Returns:\n",
    "       A pytorch compatible Dataset object\n",
    "       list of tuples.\n",
    "    \"\"\"\n",
    "    istream = open(filename)\n",
    "    istream.readline()#skips header\n",
    "    xydataset = [ ]\n",
    "    for line in istream:\n",
    "        fields = line.split(',')\n",
    "        label  = fields[0]\n",
    "        text   = ','.join(fields[1:])\n",
    "        xydataset.append( (text,label) )\n",
    "    istream.close()\n",
    "    return SentimentDataset(xydataset)\n",
    "\n",
    "train_set = load_data_set('../input/sentimentIMDB_train.csv')\n",
    "print('Loaded %d examples as train set. '%(len(train_set)))\n",
    "\n",
    "#This demonstrates the DataLoader basic usage\n",
    "print('Train data Sample (1st batch only)')\n",
    "train_loader = DataLoader(train_set, batch_size=4, shuffle=True)\n",
    "for text_batch, label_batch in train_loader:       #iterates over all batches\n",
    "    for text,label in zip(text_batch, label_batch):#iterates over example in current batch \n",
    "        print('  ',text[:50],'...',label)\n",
    "    break #stops displaying once first batch \n",
    "\n",
    "    \n",
    "test_set = load_data_set('../input/sentimentIMDB_test.csv')\n",
    "print('Loaded %d examples as test set. '%(len(test_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codage\n",
    "======\n",
    "Fonctionnalités pour associer les mots à des entiers et pour vectoriser un texte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  make_w2idx(dataset):\n",
    "    \"\"\"\n",
    "    Maps words to integers\n",
    "    Returns:\n",
    "    A dictionary mapping words to integers\n",
    "    \"\"\"\n",
    "    wordset = set([])\n",
    "    for text,label in dataset:\n",
    "        words = text.split()\n",
    "        wordset.update(words)\n",
    "    return dict(zip(wordset,range(len(wordset))))   \n",
    "\n",
    "def vectorize_text(text,w2idx):\n",
    "    counts = Counter(text.split())\n",
    "    xvec = torch.zeros(len(w2idx))\n",
    "    for word in counts:\n",
    "        if word in w2idx:       #manages unk words (ignored)\n",
    "            xvec[w2idx[word]] = counts[word] \n",
    "    return xvec.squeeze()\n",
    "\n",
    "def vectorize_target(ylabel):\n",
    "     return torch.tensor(float(ylabel))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifieur\n",
    "==========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalyzer(nn.Module): \n",
    "    \n",
    "    def __init__(self):    \n",
    "        super(SentimentAnalyzer, self).__init__()\n",
    "        self.reset_structure(1,1)\n",
    "        \n",
    "    def reset_structure(self,vocab_size, num_labels):\n",
    "        self.W = nn.Linear(vocab_size, num_labels)\n",
    "            \n",
    "    def forward(self, text_vec):    \n",
    "        return torch.sigmoid(self.W(text_vec)) #sigmoid is the logistic activation\n",
    "        \n",
    "    def train(self,train_set,learning_rate,epochs):\n",
    "            \n",
    "        self.w2idx = make_w2idx(train_set)\n",
    "        self.reset_structure(len(self.w2idx),1)\n",
    "            \n",
    "        #remind that minimizing Binary Cross Entropy <=> minimizing NLL\n",
    "        loss_func   = nn.BCELoss() \n",
    "        optimizer   = optim.SGD(self.parameters(), lr=learning_rate)\n",
    "        #We do not take advantage of the DataLoader here but we demonstrate how to use it\n",
    "        train,dev=random_split(train_set,[20000,5000])\n",
    "        #train,dev=random_split(train_set,[800,200])\n",
    "        data_loader_train = DataLoader(train, batch_size=len(train), shuffle=True)\n",
    "        data_loader_dev = DataLoader(dev, batch_size=len(dev), shuffle=True)\n",
    "\n",
    "        acc=[]\n",
    "        bestacc=0\n",
    "        bestweights=self.W.weight\n",
    "        for epoch in range(epochs):\n",
    "            global_logloss = 0.0\n",
    "            for Xbatch,Ybatch in data_loader_train: #there is a single batch,this loop does a single iteration\n",
    "                for X, Y in zip(Xbatch,Ybatch): \n",
    "                    self.zero_grad()\n",
    "                    xvec            = vectorize_text(X,self.w2idx)\n",
    "                    yvec            = vectorize_target(Y)\n",
    "                    prob            = self(xvec).squeeze()\n",
    "                    loss            = loss_func(prob,yvec)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    global_logloss += loss.item()\n",
    "            for Xbatch,Ybatch in data_loader_dev: \n",
    "                acc_=0\n",
    "                for X, Y in zip(Xbatch,Ybatch): \n",
    "                    xvec=vectorize_text(X,self.w2idx)\n",
    "                    prob=self(xvec).squeeze()\n",
    "                    if prob.round()==vectorize_target(Y) :\n",
    "                        acc_+=1\n",
    "            acc.append(acc_/len(dev))   \n",
    "            if (acc_>bestacc) : \n",
    "                bestacc=acc_\n",
    "                saved=self.W.state_dict()\n",
    "            print(\"Epoch %d, mean cross entropy = %f\"%(epoch,global_logloss/len(train)))\n",
    "        self.W.load_state_dict(saved)\n",
    "    #TODO $2 : ADD HERE A METHOD FOR PERFORMING PREDICTIONS\n",
    "    # This method has to generate a CSV file with two columns:\n",
    "    #   idx   : example identifier\n",
    "    #   sentY : predicted sentiment (0 or 1)\n",
    "    \n",
    "    def run_test(self,test_set) : \n",
    "        idx=[]\n",
    "        pred=[]\n",
    "        for text,label in test_set.xydata : \n",
    "            xvec= vectorize_text(text,self.w2idx)\n",
    "            pred.append((int)(self(xvec).squeeze().round().item()))\n",
    "            idx.append(label)\n",
    "        submission = pd.DataFrame({\"idx\" : idx ,\"SentY\" : pred})\n",
    "        #enregistre les preds\n",
    "        filename = 'DugrainCharlotte_pred_sentimentanalysis.csv'\n",
    "        submission.to_csv(filename,index=False)\n",
    "        return pred,submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inférences\n",
    "=========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, mean cross entropy = 0.554869\n",
      "Epoch 1, mean cross entropy = 0.392729\n",
      "Epoch 2, mean cross entropy = 0.328108\n",
      "Epoch 3, mean cross entropy = 0.291463\n",
      "Epoch 4, mean cross entropy = 0.263224\n",
      "Epoch 5, mean cross entropy = 0.242480\n",
      "Epoch 6, mean cross entropy = 0.223494\n",
      "Epoch 7, mean cross entropy = 0.206584\n",
      "Epoch 8, mean cross entropy = 0.191539\n",
      "Epoch 9, mean cross entropy = 0.182082\n",
      "Epoch 10, mean cross entropy = 0.172040\n",
      "Epoch 11, mean cross entropy = 0.163025\n",
      "Epoch 12, mean cross entropy = 0.153125\n",
      "Epoch 13, mean cross entropy = 0.149091\n",
      "Epoch 14, mean cross entropy = 0.141943\n",
      "Epoch 15, mean cross entropy = 0.135028\n",
      "Epoch 16, mean cross entropy = 0.130518\n",
      "Epoch 17, mean cross entropy = 0.125687\n",
      "Epoch 18, mean cross entropy = 0.121449\n",
      "Epoch 19, mean cross entropy = 0.116693\n",
      "Epoch 20, mean cross entropy = 0.111859\n",
      "Epoch 21, mean cross entropy = 0.108955\n",
      "Epoch 22, mean cross entropy = 0.105741\n",
      "Epoch 23, mean cross entropy = 0.102551\n",
      "Epoch 24, mean cross entropy = 0.100250\n",
      "Epoch 25, mean cross entropy = 0.096816\n",
      "Epoch 26, mean cross entropy = 0.094329\n",
      "Epoch 27, mean cross entropy = 0.092658\n",
      "Epoch 28, mean cross entropy = 0.089349\n",
      "Epoch 29, mean cross entropy = 0.087027\n",
      "Epoch 30, mean cross entropy = 0.085260\n",
      "Epoch 31, mean cross entropy = 0.083132\n",
      "Epoch 32, mean cross entropy = 0.080986\n",
      "Epoch 33, mean cross entropy = 0.079365\n",
      "Epoch 34, mean cross entropy = 0.077860\n",
      "Epoch 35, mean cross entropy = 0.075966\n",
      "Epoch 36, mean cross entropy = 0.074099\n",
      "Epoch 37, mean cross entropy = 0.072579\n",
      "Epoch 38, mean cross entropy = 0.071640\n",
      "Epoch 39, mean cross entropy = 0.069702\n",
      "Epoch 40, mean cross entropy = 0.068520\n",
      "Epoch 41, mean cross entropy = 0.067069\n",
      "Epoch 42, mean cross entropy = 0.065931\n",
      "Epoch 43, mean cross entropy = 0.064828\n",
      "Epoch 44, mean cross entropy = 0.063517\n"
     ]
    }
   ],
   "source": [
    "sent = SentimentAnalyzer()\n",
    "#plusieurs hyperparamètres testés, il faudrait plus de temps/puissance pour faire une recherche plus poussée/systématique\n",
    "sent.train(train_set,0.003,45)\n",
    "#sent.train(train_set[:1000],0.01,4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred,submission=sent.run_test(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>SentY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  idx  SentY\n",
       "0   0      1\n",
       "1   1      1\n",
       "2   2      1\n",
       "3   3      1\n",
       "4   4      1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
